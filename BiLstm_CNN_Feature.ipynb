{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a650c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18b280b6",
    "outputId": "c571a747-774c-4b16-e5d2-4bd454fad830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draft', '.DS_Store', 'BiLstm_CNN_Feature.ipynb', 'dataset', 'CNN_IDSGAN.ipynb', 'Preprocess_41dim_testing_training_Final.ipynb', 'CNN_Feature.ipynb', '.ipynb_checkpoints']\n",
      "Position of Functional Features\n",
      "   {'DoS': [0, 1, 2, 6, 7, 8, 24, 27, 28], 'U2R': [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21], 'Probe': [0, 1, 2, 6, 7, 8, 24, 28, 31, 32, 36, 39], 'R2L': [0, 1, 2, 4, 7, 8, 12, 13, 14, 15, 17, 19, 20]}\n",
      "Position of Functional Features:\n",
      "   {'DoS': [3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], 'U2R': [1, 3, 13, 14, 17, 18, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], 'Probe': [3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 29, 30, 33, 34, 35, 37, 38, 40], 'R2L': [3, 5, 6, 9, 10, 11, 16, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 09:06:26.559144: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amout of Generator Trainset: 42\n",
      "Amout of Discriminator Trainset: 41\n",
      "    Loaded IDS Model From: dataset/Saved_Model/IDSModel/BiLstm_CNN.h5\n"
     ]
    }
   ],
   "source": [
    "# # Adversarial Attack on Black-box IDS using GAN\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# Libs for Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# ### Path configuration && Processing the dataset for generator and discriminator\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Loading train set---\n",
    "1. g_trainset: Generator trainset\n",
    "2. d_trainset: Discriminator trainset\n",
    "\n",
    "'''\n",
    "print(os.listdir())\n",
    "# Base Path\n",
    "Dataset_Path = \"dataset/\"\n",
    "SavedModelPath = Dataset_Path + \"Saved_Model/\"\n",
    "\n",
    "# Dataset Path\n",
    "\n",
    "Trainsets_Path = Dataset_Path + 'Trainset/'\n",
    "ids_trainset_path = Trainsets_Path + \"training_data.csv\"\n",
    "testset_path = Dataset_Path + \"Testset/\" + \"testing_data.csv\"\n",
    "d_trainset_path = Trainsets_Path + \"GAN-D.csv\"\n",
    "g_trainset_path = Trainsets_Path + \"GAN-G.csv\"\n",
    "\n",
    "# The training set is divided into normal traffic records and malicious traffic records\n",
    "df = pd.read_csv(ids_trainset_path)\n",
    "new_train_df = pd.read_csv(ids_trainset_path)\n",
    "new_train_df[\"Class\"] = new_train_df[\"Class\"].map(lambda x: 0 if x == \"Normal\" else 1)\n",
    "\n",
    "\n",
    "d_train_data = df[df[\"Class\"] == \"Normal\"]\n",
    "d_train_data.to_csv(os.path.join(Trainsets_Path, 'GAN-D.csv'), index=False)\n",
    "g_train_data = df[df[\"Class\"] != \"Normal\"]\n",
    "g_train_data.to_csv(os.path.join(Trainsets_Path, 'GAN-G.csv'), index=False)\n",
    "\n",
    "# GAN Saved Models Paths\n",
    "GAN_Save_Path = SavedModelPath + 'GANModel/'\n",
    "\n",
    "# Blackbox IDS\n",
    "IDS_ModelPath = SavedModelPath + \"IDSModel/BiLstm_CNN.h5\"\n",
    "os.listdir(SavedModelPath)\n",
    "\n",
    "\n",
    "# ### Create dictionary of Fun/Non-fun features for eatch attack category\n",
    "#\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "N_FEATURES = 41\n",
    "\n",
    "ATTACK_CATEGORIES = ['DoS', 'U2R', 'Probe', 'R2L']\n",
    "DOS_FEATURES = [0, 1, 2, 6, 7, 8, 24, 27, 28]\n",
    "U2R_FEATURES = [0, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 19, 20, 21]\n",
    "R2L_FEATURES = [0, 1, 2, 4, 7, 8, 12, 13, 14, 15, 17, 19, 20]\n",
    "PROBE_FEATURES = [0, 1, 2, 6, 7, 8, 24, 28, 31, 32, 36, 39]\n",
    "\n",
    "POS_FUNCTIONAL_FEATURES = {'DoS': DOS_FEATURES,\n",
    "                           'U2R': U2R_FEATURES,\n",
    "                           'Probe': PROBE_FEATURES,\n",
    "                           'R2L': R2L_FEATURES}\n",
    "\n",
    "POS_NONFUNCTIONAL_FEATURES = {}\n",
    "for attack_category, pos_functional_feature in POS_FUNCTIONAL_FEATURES.items():\n",
    "    pos_nonfunctional_feature = []\n",
    "    for i in range(N_FEATURES):\n",
    "        if i not in pos_functional_feature:\n",
    "            pos_nonfunctional_feature.append(i)\n",
    "    POS_NONFUNCTIONAL_FEATURES[attack_category] = pos_nonfunctional_feature\n",
    "\n",
    "print('Position of Functional Features\\n  ', POS_FUNCTIONAL_FEATURES)\n",
    "print('Position of Functional Features:\\n  ', POS_NONFUNCTIONAL_FEATURES)\n",
    "\n",
    "\n",
    "# ### Create training set for each attack category\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "'''\n",
    "1. nonfunc_set_G : dictionaries to store the nonfunctional contents for generator crafting.\n",
    "2. raw_attack_categories : dictionaries to store the origin contents of each attack categories.\n",
    "3. normal: All normal record in d_train_data without labels\n",
    "\n",
    "'''\n",
    "\n",
    "def create_batch2(x,batch_size):\n",
    "    a = list(range(len(x)))\n",
    "    np.random.shuffle(a)\n",
    "    x = x[a]\n",
    "    batch_x = [x[batch_size * i : (i+1)*batch_size,:] for i in range(len(x)//batch_size)]\n",
    "    return np.array(batch_x)\n",
    "\n",
    "\n",
    "def preprocess_adversarial_data(g_trainset, attack_category):\n",
    "    attack_data = g_trainset[g_trainset[\"Class\"] == attack_category]\n",
    "    del attack_data[\"Class\"]\n",
    "    #attack_data = attack_data.sample(frac=0.5, random_state=7)\n",
    "    return np.array(attack_data)\n",
    "\n",
    "\n",
    "\n",
    "#  (27, 41)\n",
    "#  (14, 41)\n",
    "#  (17, 41)\n",
    "\n",
    "\n",
    "\n",
    "# ### GAN DEFINATION\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "            G-input is nonfunctional feature, G-out is adversarial nonfuncitonal feature.\n",
    "            --> input_dim = output_dim = len(nonfunctional_features)\n",
    "'''\n",
    "def Generator_v1(input_size, output_size):\n",
    "    #  TODO\n",
    "    inputx = tf.keras.Input(shape=(input_size,))\n",
    "\n",
    "    feat = layers.Dense(32)(inputx)\n",
    "    feat = layers.BatchNormalization(momentum=0.8)(feat)\n",
    "    feat = layers.Activation('relu')(feat)\n",
    "    feat = layers.LeakyReLU(0.2)(feat)\n",
    "\n",
    "    feat = layers.Dense(64)(feat)\n",
    "    feat = layers.BatchNormalization(momentum=0.8)(feat)\n",
    "    feat = layers.Activation('relu')(feat)\n",
    "    feat = layers.LeakyReLU(0.2)(feat)\n",
    "\n",
    "    feat = layers.Dense(128)(feat)\n",
    "    feat = layers.BatchNormalization(momentum=0.8)(feat)\n",
    "    feat = layers.Activation('relu')(feat)\n",
    "    feat = layers.LeakyReLU(0.2)(feat)\n",
    "\n",
    "    feat = layers.Dense(256)(feat)\n",
    "    feat = layers.BatchNormalization(momentum=0.8)(feat)\n",
    "    feat = layers.Activation('relu')(feat)\n",
    "    feat = layers.LeakyReLU(0.2)(feat)\n",
    "\n",
    "    feat = layers.Dense(np.prod(input_size,))(feat)\n",
    "    out = layers.Activation('sigmoid')(feat)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputx, outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "def Generator(input_size, output_size):\n",
    "    #  TODO\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(32, input_shape=(input_size,)))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "    model.add(layers.Dense(np.prod(input_size,)))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "def init_generator(attack_category):\n",
    "    input_dim = len(POS_NONFUNCTIONAL_FEATURES[attack_category])\n",
    "    output_dim = input_dim\n",
    "    generator = Generator_v1(input_dim, output_dim)\n",
    "    return generator\n",
    "\n",
    "def gen_adversarial_attack(generator, noise_dim, raw_attack, attack_category):\n",
    "\n",
    "    # get nonfunctional features val\n",
    "    batch_size = len(raw_attack)\n",
    "    noise = tf.convert_to_tensor(np.random.uniform(0,1,(batch_size, noise_dim)), dtype=np.float64)\n",
    "    generator_out = generator(noise)\n",
    "\n",
    "    pos_nonfunctional_feature = POS_NONFUNCTIONAL_FEATURES[attack_category]\n",
    "    not_pos_nonfunctional_feature = [x for x in list(range(raw_attack.shape[1])) if x not in pos_nonfunctional_feature]\n",
    "    shuffled_pos = not_pos_nonfunctional_feature + pos_nonfunctional_feature\n",
    "\n",
    "    p_matrix = np.zeros((len(shuffled_pos), len(shuffled_pos))) #[41, 41]\n",
    "    for i in range(p_matrix.shape[0]):\n",
    "        p_matrix[i, shuffled_pos[i]] = 1\n",
    "\n",
    "    keep_attack = np.zeros((raw_attack.shape[0], len(not_pos_nonfunctional_feature)), dtype=np.float32) #[64,17]\n",
    "    for idx in range(raw_attack.shape[0]):\n",
    "        for j in range(len(not_pos_nonfunctional_feature)):\n",
    "            keep_attack[idx, j] = raw_attack[idx][not_pos_nonfunctional_feature[j]]\n",
    "\n",
    "    adversarial_attack = tf.concat([keep_attack, generator_out], axis=1)\n",
    "    adversarial_attack = tf.linalg.matmul(tf.constant(p_matrix, dtype=tf.float32), adversarial_attack, transpose_b=True)\n",
    "    adversarial_attack = tf.transpose(adversarial_attack)\n",
    "\n",
    "    return adversarial_attack, generator_out\n",
    "\n",
    "def Discriminator(input_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, input_shape=(input_size, )))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ### Similarity analysis Code\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Hyber Parameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "CRITIC_ITERS = 2\n",
    "MAX_EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "# GAN-D\n",
    "D_INPUT_DIM = N_FEATURES\n",
    "discriminator = Discriminator(D_INPUT_DIM)\n",
    "\n",
    "## Prepare Dataset\n",
    "g_train_data = pd.read_csv(g_trainset_path)\n",
    "d_train_data = pd.read_csv(d_trainset_path)\n",
    "#d_train_data = d_train_data.sample(frac=0.1, random_state=7)\n",
    "\n",
    "# All normal record in train_data\n",
    "\n",
    "del d_train_data[\"Class\"]\n",
    "normal = np.array(d_train_data)\n",
    "\n",
    "print(\"Amout of Generator Trainset:\", g_train_data.shape[1])\n",
    "print(\"Amout of Discriminator Trainset:\", d_train_data.shape[1])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_ids_model():\n",
    "    ids_model = tf.keras.models.load_model(IDS_ModelPath)\n",
    "    print(f\"{4*' '}Loaded IDS Model From: {IDS_ModelPath}\")\n",
    "    return ids_model\n",
    "\n",
    "ids_model = load_ids_model()\n",
    "\n",
    "\n",
    "def generator_loss(d_generated):\n",
    "    return tf.reduce_mean(d_generated) * 1\n",
    "\n",
    "def discriminator_loss(d_normal, d_generated, predicted_attacks, predicted_normals):\n",
    "    loss_attack = tf.math.reduce_mean(predicted_attacks * d_generated)\n",
    "    loss_normal = tf.math.reduce_mean(predicted_normals * d_normal)\n",
    "    d_loss = tf.math.subtract(loss_normal, loss_attack)\n",
    "    return d_loss, loss_attack, loss_normal\n",
    "\n",
    "def _setTheshold(arr):\n",
    "    a = tf.where(tf.less(arr, 0.0), 0, arr) \n",
    "    arr = tf.where(tf.greater(a, 1.0), 1, a)\n",
    "  \n",
    "    return arr\n",
    "\n",
    "def train_discriminator(discriminator, ids_model, generator, critic_iters, optimizer_D, normal_b, noise_dim, attack_traffic, attack_category):\n",
    "    run_d_loss = 0\n",
    "    run_d_loss_normal = 0\n",
    "    run_d_loss_attack = 0\n",
    "    cnt = 0\n",
    "    for c in range(critic_iters):\n",
    "\n",
    "        # GAN-G Generate Adversarial Attack\n",
    "        adversarial_attack, generator_out = gen_adversarial_attack(generator, noise_dim, attack_traffic, attack_category)\n",
    "\n",
    "        # Make data to feed IDS\n",
    "        \n",
    "        adversarial_attack = tf.cast(adversarial_attack, tf.float32)\n",
    "        normal_b = tf.cast(normal_b, tf.float32)\n",
    "        \n",
    "        ids_input = tf.concat([adversarial_attack, normal_b],0).numpy()\n",
    "        ids_input = np.reshape(ids_input, (ids_input.shape[0], ids_input.shape[1], 1))\n",
    "        ids_pred_label = (ids_model.predict(tf.expand_dims(ids_input, axis=-1)) > 0.5).astype(\"int8\").flatten()\n",
    "        #ids_pred_label = tf.math.argmax(ids_model.predict(tf.expand_dims(ids_input, axis=-1)), axis=1).numpy()\n",
    "        #ids_pred_label = ids_model.predict(ids_input)\n",
    "\n",
    "        pred_normal = ids_input[ids_pred_label == 0]\n",
    "        pred_attack = ids_input[ids_pred_label == 1]\n",
    "\n",
    "\n",
    "        pred_normal = tf.convert_to_tensor(pred_normal, dtype=np.float32)\n",
    "        pred_attack = tf.convert_to_tensor(pred_attack, dtype=np.float32)\n",
    "\n",
    "        with tf.GradientTape(persistent = False) as Dtape:\n",
    "            D_normal = discriminator(pred_normal)\n",
    "            D_attack= discriminator(pred_attack)\n",
    "\n",
    "            loss_normal = tf.reduce_sum(tf.math.square(D_normal))/2\n",
    "            loss_attack = tf.reduce_sum(tf.math.square(D_attack-1))/2\n",
    "            #loss_normal = tf.reduce_mean(D_normal)\n",
    "            #loss_attack = tf.reduce_mean(D_attack)\n",
    "            d_loss = loss_attack + loss_normal\n",
    "            #d_loss, loss_attack, loss_normal = discriminator_loss(D_normal, D_attack, pred_attack, pred_normal)\n",
    "\n",
    "        dGradients = Dtape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        optimizer_D.apply_gradients(zip(dGradients, discriminator.trainable_variables))\n",
    "\n",
    "        run_d_loss += d_loss.numpy()\n",
    "        run_d_loss_normal += loss_normal.numpy()\n",
    "        run_d_loss_attack += loss_attack.numpy()\n",
    "\n",
    "        return run_d_loss, run_d_loss_normal, run_d_loss_attack, cnt\n",
    "\n",
    "def train_generator(generator, discriminator, optimizer_G, noise_dim, attack_traffic, attack_category):\n",
    "    #  for p in discriminator.layers:\n",
    "    #      p.trainable = False\n",
    "    #\n",
    "    #  discriminator.trainable = False\n",
    "\n",
    "    #D_pred = tf.constant(D_pred, dtype=tf.float32)\n",
    "    #  with tf.GradientTape(persistent = False) as Gtape:\n",
    "\n",
    "    with tf.GradientTape() as Gtape:\n",
    "        adversarial_attack, generator_out = gen_adversarial_attack(generator, noise_dim, attack_traffic, attack_category)\n",
    "        D_pred = discriminator(adversarial_attack)\n",
    "        #  Gtape.watch(D_pred)\n",
    "        g_loss = tf.reduce_sum(tf.math.square(D_pred))/2\n",
    "        #g_loss = -tf.reduce_mean(D_pred)\n",
    "        #g_loss = generator_loss(D_pred)\n",
    "\n",
    "    gGradients = Gtape.gradient(g_loss, generator.trainable_variables)\n",
    "    optimizer_G.apply_gradients(zip(gGradients, generator.trainable_variables))\n",
    "\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b280b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18b280b6",
    "outputId": "c571a747-774c-4b16-e5d2-4bd454fad830"
   },
   "outputs": [],
   "source": [
    "def cal_dr(ids_model, normal, raw_attack, adversarial_attack):\n",
    "    \n",
    "    # Make data to feed IDS contain: Attack & Normal\n",
    "    raw_attack = tf.cast(raw_attack, tf.float32)\n",
    "    normal = tf.cast(normal, tf.float32)\n",
    "        \n",
    "    o_ids_input = tf.concat([raw_attack, normal],0).numpy()\n",
    "    a_ids_input = tf.concat([adversarial_attack, normal], 0).numpy()\n",
    "    \n",
    "    # Shuffle Input\n",
    "    #l = list(range(len(a_ids_input)))\n",
    "    #np.random.shuffle(l)\n",
    "    #o_ids_input = o_ids_input[l]\n",
    "    #a_ids_input = a_ids_input[l]\n",
    "\n",
    "    #o_pred_label = tf.math.argmax(ids_model.predict(tf.expand_dims(o_ids_input, axis=-1)), axis=1).numpy()\n",
    "    #a_pred_label = tf.math.argmax(ids_model.predict(tf.expand_dims(a_ids_input, axis=-1)), axis=1).numpy()\n",
    "    o_ids_input = np.reshape(o_ids_input, (o_ids_input.shape[0], o_ids_input.shape[1], 1))\n",
    "    o_pred_label = (ids_model.predict(tf.expand_dims(o_ids_input, axis=-1)) > 0.5).astype(\"int8\").flatten()\n",
    "    a_ids_input = np.reshape(a_ids_input, (a_ids_input.shape[0], a_ids_input.shape[1], 1))\n",
    "    a_pred_label = (ids_model.predict(tf.expand_dims(a_ids_input, axis=-1)) > 0.5).astype(\"int8\").flatten()\n",
    "\n",
    "\n",
    "\n",
    "    # True Label\n",
    "    ids_true_label = np.r_[np.ones(BATCH_SIZE),np.zeros(BATCH_SIZE)]\n",
    "    ids_true_label = ids_true_label.reshape(-1,1)\n",
    "\n",
    "    # Calc DR\n",
    "    tn1, fn1, fp1, tp1 = confusion_matrix(ids_true_label,o_pred_label).ravel()\n",
    "    tn2, fn2, fp2, tp2 = confusion_matrix(ids_true_label,a_pred_label).ravel()\n",
    "    origin_dr = tp1/(tp1 + fp1)\n",
    "    adversarial_dr = tp2/(tp2 + fp2)\n",
    "    \n",
    "    return origin_dr, adversarial_dr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6EZQUsIdKBBk",
   "metadata": {
    "id": "6EZQUsIdKBBk"
   },
   "outputs": [],
   "source": [
    "EVASION_RATE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc061889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of KDDTest-21: \t\t11850\n",
      "Amount of Normal:\t\t2330 (36 batchs - 64 records/batch)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11850, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_generated_dataset(df, path):\n",
    "    df.to_csv(path, index = False, header=True)\n",
    "    print(f\"\t    Generated Dataset Saved\\n\t    Saved Path: {path}\")\n",
    "\n",
    "generated_dataset_path = Dataset_Path + 'Generated_set/Feature-GAN'\n",
    "testset = pd.read_csv(testset_path)\n",
    "DATASET_COLUMNS = testset.columns[0:41]\n",
    "print(f\"Amount of KDDTest-21: \\t\\t{len(testset)}\")\n",
    "# test_normal\n",
    "test_normal = np.array(testset[testset[\"Class\"] == 'Normal'])[:,:-1]\n",
    "# Create batch of normal traffic\n",
    "test_batch_normal = create_batch2(test_normal,BATCH_SIZE).astype('float64')\n",
    "print(f\"Amount of Normal:\\t\\t{len(test_normal)} ({len(test_batch_normal)} batchs - {BATCH_SIZE} records/batch)\")\n",
    "\n",
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KUcV0yF9J3Dw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KUcV0yF9J3Dw",
    "outputId": "e5366576-f879-4c1b-86fa-cb29f2653a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== TRAINING GAN ========================================\n",
      "================================================================================\n",
      " IDS Model: BiLstm\n",
      "--------------------------------------------------------------------------------\n",
      "    Attack Category: DoS\n",
      "    nf              : 32 (num. of nonfunctional features)\n",
      "    GAN-G NOISE DIM : 32\n",
      "    GAN-G INPUT DIM : 32\n",
      "    GAN-G OUTPUT DIM: 32\n",
      "==>IDSGAN start training\n",
      "    Folder for saving GAN Models : dataset/Saved_Model/GANModel/\n",
      "     Epoch          G-Loss          D-Loss     Normal-loss     Benign-loss         O-DR(%)         A-DR(%)      Runtime(s)\n",
      "         1      10.5594845        7.262874        4.306381        2.956493      100.000000      100.000000        2.657516\n",
      "         1       10.562349        7.238585        4.209537        3.029049      100.000000      100.000000        1.132236\n",
      "         1       10.581218        7.228362        4.215543        3.012820       99.479167      100.000000        1.150551\n",
      "         1       10.574026        7.247275        4.261187        2.986088       99.609375      100.000000        1.093851\n",
      "         1       10.588846        7.249052        4.276399        2.972653       99.062500      100.000000        1.125029\n",
      "         1       10.581258        7.256469        4.295151        2.961318       99.218750      100.000000        1.094973\n",
      "         1       10.596208        7.245863        4.284244        2.961620       99.107143      100.000000        1.053956\n",
      "         1       10.609013        7.238446        4.270580        2.967866       99.218750      100.000000        1.097533\n",
      "         1      10.6129875        7.233207        4.252749        2.980459       99.305556      100.000000        1.096253\n",
      "         1       10.622333        7.226416        4.249477        2.976939       99.218750      100.000000        1.203178\n",
      "         1       10.626815        7.215736        4.234554        2.981182       99.289773      100.000000        1.074087\n",
      "         1       10.637513        7.209819        4.228248        2.981571       99.218750      100.000000        1.078553\n",
      "         1       10.643049        7.207129        4.231987        2.975142       99.278846      100.000000        1.071467\n",
      "         1       10.643443        7.210196        4.238984        2.971212       99.330357      100.000000        1.087171\n",
      "         1       10.653666        7.210253        4.249649        2.960604       99.375000      100.000000        1.066464\n",
      "         1      10.6622925        7.207788        4.249294        2.958494       99.316406      100.000000        1.337282\n",
      "         1       10.663256        7.205616        4.246770        2.958845       99.356618      100.000000        1.204097\n",
      "         1      10.6661215        7.205875        4.250645        2.955230       99.392361      100.000000        1.411410\n",
      "         1       10.671323        7.204167        4.250307        2.953860       99.424342      100.000000        1.656262\n",
      "         1       10.675715        7.198527        4.240737        2.957790       99.453125      100.000000        1.374527\n",
      "         1       10.678033        7.197371        4.239929        2.957442       99.479167      100.000000        1.378003\n",
      "         1       10.681746        7.196260        4.242413        2.953847       99.502841      100.000000        1.114879\n",
      "         1       10.686895        7.192286        4.242380        2.949906       99.456522      100.000000        1.086988\n",
      "         1       10.690612        7.191020        4.242326        2.948694       99.479167      100.000000        1.139304\n",
      "         1       10.696466        7.187678        4.240661        2.947017       99.500000      100.000000        1.232982\n",
      "         1       10.700184        7.185635        4.237670        2.947965       99.459135      100.000000        1.310181\n",
      "         1       10.701912        7.184204        4.240699        2.943505       99.479167      100.000000        1.366101\n",
      "         1       10.707771        7.181496        4.241513        2.939983       99.497768      100.000000        1.396315\n",
      "         1       10.712924        7.178072        4.238184        2.939888       99.461207      100.000000        1.167521\n",
      "         1      10.7182665        7.174679        4.239921        2.934758       99.479167      100.000000        1.200966\n",
      "         1        10.72482        7.170464        4.239427        2.931037       99.445565      100.000000        1.170478\n",
      "         1       10.729471        7.167318        4.235905        2.931414       99.462891      100.000000        1.170084\n",
      "         1        10.73113        7.167339        4.239374        2.927965       99.479167      100.000000        1.165238\n",
      "         1       10.736832        7.164178        4.237667        2.926511       99.494485      100.000000        1.319419\n",
      "         1       10.740954        7.162012        4.238157        2.923854       99.508929      100.000000        1.536662\n",
      "         1       10.744843        7.159994        4.234030        2.925964       99.479167      100.000000        1.461574\n",
      "         1       10.749428        7.156720        4.230506        2.926214       99.493243      100.000000        1.779058\n",
      "         1       10.752896        7.155698        4.231950        2.923748       99.465461      100.000000        2.004150\n",
      "         1       10.756986        7.154255        4.233002        2.921253       99.439103      100.000000        1.191683\n",
      "         1       10.759463        7.152611        4.230528        2.922082       99.414062      100.000000        1.132087\n",
      "         1       10.763229        7.150670        4.229436        2.921234       99.390244      100.000000        1.360404\n",
      "         1       10.766558        7.149518        4.231812        2.917707       99.404762      100.000000        1.582253\n",
      "         1       10.771023        7.147749        4.227342        2.920407       99.418605      100.000000        1.565759\n",
      "         1       10.774759        7.143861        4.219386        2.924475       99.396307      100.000000        1.375096\n",
      "         1       10.777772        7.141137        4.214222        2.926915       99.409722      100.000000        1.093197\n",
      "         1       10.779973        7.139110        4.212988        2.926122       99.422554      100.000000        1.168637\n",
      "         1       10.783278        7.137039        4.210146        2.926893       99.401596      100.000000        1.108050\n",
      "         1        10.78672        7.134249        4.207682        2.926566       99.414062      100.000000        1.176028\n",
      "         1       10.790667        7.132084        4.206960        2.925124       99.426020      100.000000        1.467696\n",
      "         1       10.795554        7.128783        4.205456        2.923327       99.437500      100.000000        1.130907\n",
      "         1       10.799254        7.127157        4.206264        2.920892       99.448529      100.000000        1.161279\n",
      "         1       10.802189        7.123489        4.198434        2.925055       99.459135      100.000000        1.205973\n",
      "         1       10.805784        7.121856        4.199196        2.922661       99.469340      100.000000        1.210326\n",
      "         1       10.808734        7.119940        4.198511        2.921430       99.479167      100.000000        1.136988\n",
      "         1       10.812131        7.117674        4.197532        2.920142       99.488636      100.000000        1.179258\n",
      "         1       10.815452        7.114579        4.196669        2.917910       99.497768      100.000000        1.267357\n",
      "         1       10.818755        7.112812        4.197035        2.915777       99.506579      100.000000        1.473307\n",
      "         1       10.821839        7.109687        4.191382        2.918305       99.515086      100.000000        1.561806\n",
      "         1       10.826174        7.107526        4.191048        2.916478       99.523305      100.000000        1.401624\n",
      "         1       10.828939        7.105422        4.190916        2.914506       99.531250      100.000000        1.743675\n",
      "         1       10.833181        7.102874        4.189894        2.912980       99.513320      100.000000        1.237377\n",
      "         1       10.837856        7.100074        4.188845        2.911229       99.495968      100.000000        1.798606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1       10.841319        7.097896        4.187186        2.910710       99.503968      100.000000        1.482662\n",
      "         1       10.845473        7.094891        4.184042        2.910849       99.511719      100.000000        1.291567\n",
      "         1       10.848987        7.093258        4.184577        2.908681       99.519231      100.000000        1.352204\n",
      "         1       10.852418        7.091097        4.182055        2.909042       99.526515      100.000000        1.925185\n",
      "         1       10.856718        7.088823        4.181432        2.907392       99.510261      100.000000        3.093057\n",
      "         1       10.861177        7.086254        4.179762        2.906492       99.494485      100.000000        1.731255\n",
      "         1       10.864623        7.084283        4.179921        2.904362       99.501812      100.000000        1.748925\n",
      "         1         10.8675        7.082905        4.179695        2.903210       99.486607      100.000000        1.548252\n",
      "         1      10.8708515        7.081829        4.180630        2.901200       99.493838      100.000000        1.522613\n",
      "         1       10.874191        7.079003        4.177465        2.901538       99.500868      100.000000        1.683079\n",
      "         1       10.877462        7.076407        4.173797        2.902611       99.507705      100.000000        1.801629\n",
      "         1       10.881864        7.074319        4.173398        2.900921       99.493243      100.000000        1.556036\n",
      "         1       10.885593        7.071841        4.171757        2.900084       99.500000      100.000000        1.694782\n",
      "         1       10.889088        7.069832        4.171830        2.898002       99.486020      100.000000        1.758903\n",
      "         1       10.893658        7.068214        4.173354        2.894859       99.452110      100.000000        1.685668\n",
      "         1      10.8967085        7.066605        4.172277        2.894328       99.459135      100.000000        1.737879\n",
      "         1       10.899708        7.064203        4.169715        2.894489       99.465981      100.000000        1.618828\n",
      "         1       10.902995        7.062123        4.168956        2.893167       99.472656      100.000000        1.789909\n",
      "         1      10.9073305        7.059646        4.168796        2.890850       99.459877      100.000000        1.763612\n",
      "         1       10.910914        7.057375        4.167288        2.890087       99.447409      100.000000        1.701894\n",
      "         1       10.913954        7.055424        4.166021        2.889403       99.454066      100.000000        1.586004\n",
      "         1       10.917465        7.053954        4.166481        2.887473       99.460565      100.000000        2.131954\n",
      "         1       10.921705        7.051596        4.164998        2.886598       99.466912      100.000000        1.840162\n",
      "         1       10.924369        7.049532        4.163340        2.886192       99.473110      100.000000        1.576578\n",
      "         1       10.928335        7.047037        4.161718        2.885319       99.461207      100.000000        1.427934\n",
      "         1       10.930949        7.044839        4.157329        2.887510       99.449574      100.000000        1.730191\n",
      "         1       10.933929        7.043190        4.157492        2.885698       99.438202      100.000000        2.084415\n",
      "         1       10.937077        7.041451        4.157597        2.883854       99.444444      100.000000        2.199883\n",
      "         1       10.941127        7.039908        4.157006        2.882901       99.450549      100.000000        1.522226\n",
      "         1       10.944608        7.037890        4.155732        2.882159       99.456522      100.000000        1.694589\n",
      "         1        10.94913        7.035357        4.154376        2.880981       99.445565      100.000000        1.722096\n",
      "         1       10.952322        7.033422        4.153120        2.880302       99.451463      100.000000        1.839342\n",
      "         1       10.956032        7.031488        4.153193        2.878295       99.440789      100.000000        1.586520\n",
      "         1      10.9598465        7.029358        4.151970        2.877388       99.446615      100.000000        1.284736\n",
      "         1       10.963372        7.027133        4.151613        2.875520       99.436211      100.000000        1.373964\n",
      "         1       10.966727        7.024951        4.150065        2.874887       99.441964      100.000000        1.738425\n",
      "         1       10.971065        7.022341        4.148464        2.873877       99.447601      100.000000        1.669491\n",
      "         1       10.974502        7.020228        4.148236        2.871993       99.453125      100.000000        1.750648\n",
      "         1       10.977427        7.018128        4.147240        2.870888       99.458540      100.000000        1.623030\n",
      "         1       10.980786        7.016173        4.146572        2.869601       99.463848      100.000000        1.549486\n",
      "         1       10.983983        7.014324        4.145957        2.868367       99.453883      100.000000        1.275803\n",
      "         1       10.987274        7.012065        4.143792        2.868273       99.459135      100.000000        1.201051\n",
      "         1       10.990623        7.009782        4.142312        2.867471       99.464286      100.000000        1.172309\n",
      "         1       10.993983        7.007833        4.142214        2.865619       99.454599      100.000000        1.165575\n",
      "         1       10.998008        7.005368        4.140777        2.864591       99.459696      100.000000        1.453418\n",
      "         1       11.001733        7.003099        4.139895        2.863204       99.464699      100.000000        1.775999\n",
      "         1       11.005424        7.000481        4.138256        2.862225       99.469610      100.000000        2.159404\n",
      "         1       11.008963        6.998543        4.138162        2.860380       99.474432      100.000000        2.319423\n",
      "         1      11.0124445        6.996511        4.137998        2.858513       99.479167      100.000000        1.752447\n",
      "         1       11.016564        6.994396        4.136878        2.857518       99.483817      100.000000        2.005935\n",
      "         1       11.020449        6.992388        4.136273        2.856115       99.488385      100.000000        1.858033\n",
      "         1       11.024044        6.990050        4.135028        2.855022       99.479167      100.000000        1.959551\n",
      "         1       11.027361        6.987847        4.134570        2.853277       99.483696      100.000000        1.420199\n",
      "         1        11.03049        6.986159        4.135014        2.851145       99.488147      100.000000        1.590607\n",
      "         1       11.033907        6.984528        4.135108        2.849420       99.492521      100.000000        1.213536\n",
      "         1       11.037375        6.982638        4.135445        2.847193       99.483581      100.000000        1.565377\n",
      "         1       11.041069        6.980131        4.134842        2.845289       99.474790      100.000000        1.277820\n",
      "         1       11.044753        6.978245        4.135219        2.843025       99.466146      100.000000        1.529050\n",
      "         1       11.047871        6.976368        4.134047        2.842321       99.444731      100.000000        1.798723\n",
      "         1       11.051363        6.973954        4.132836        2.841119       99.449283      100.000000        1.428222\n",
      "         1       11.054715        6.971836        4.132371        2.839465       99.453760      100.000000        1.288055\n",
      "         1       11.058382        6.969490        4.130960        2.838530       99.458165      100.000000        1.352257\n",
      "         1       11.061639        6.967897        4.131102        2.836795       99.462500      100.000000        1.307831\n",
      "         1        11.06514        6.965658        4.129635        2.836023       99.466766      100.000000        1.333431\n",
      "         1       11.069188        6.962887        4.126950        2.835936       99.470965      100.000000        1.563573\n",
      "         1        11.07232        6.960529        4.125723        2.834806       99.450684      100.000000        1.416717\n",
      "         1       11.075614        6.958673        4.125020        2.833653       99.454942      100.000000        2.251783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1       11.078915        6.956750        4.125158        2.831592       99.459135      100.000000        2.089005\n",
      "         1       11.081543        6.954932        4.123830        2.831102       99.451336      100.000000        1.909834\n",
      "         1       11.085099        6.952577        4.123599        2.828978       99.455492      100.000000        1.724285\n",
      "         1       11.088415        6.950256        4.122418        2.827838       99.447838      100.000000        1.811155\n",
      "         1       11.091585        6.948212        4.121484        2.826727       99.440299      100.000000        2.714166\n",
      "         1       11.094855        6.946128        4.121385        2.824743       99.444444      100.000000        1.539947\n",
      "         1      11.0985775        6.943667        4.118993        2.824674       99.448529      100.000000        1.397335\n",
      "         1       11.102409        6.941226        4.117082        2.824144       99.452555      100.000000        1.584514\n",
      "         1        11.10569        6.938857        4.115462        2.823395       99.445199      100.000000        2.552574\n",
      "         1       11.109124        6.936491        4.113877        2.822615       99.449191      100.000000        2.026660\n",
      "         1       11.113206        6.934069        4.112886        2.821184       99.453125      100.000000        1.765665\n",
      "         1       11.116457        6.932369        4.112743        2.819626       99.457004      100.000000        1.197556\n",
      "         1       11.120117        6.930092        4.111340        2.818752       99.460827      100.000000        1.670789\n",
      "         1       11.123479        6.927980        4.110396        2.817584       99.453671      100.000000        1.254416\n",
      "         1       11.126926        6.925651        4.109263        2.816387       99.457465      100.000000        1.241846\n",
      "         1       11.130782        6.923978        4.109371        2.814607       99.461207      100.000000        1.211595\n",
      "         1       11.134299        6.921211        4.108301        2.812910       99.464897      100.000000        1.187701\n",
      "         1       11.137222        6.919092        4.106064        2.813028       99.468537      100.000000        1.212177\n",
      "         1       11.140665        6.916795        4.105379        2.811416       99.461571      100.000000        1.191344\n",
      "         1       11.144038        6.914487        4.104249        2.810238       99.465185      100.000000        1.188814\n",
      "         1       11.147834        6.912297        4.103734        2.808563       99.458333      100.000000        1.206009\n",
      "         1       11.151047        6.910237        4.102827        2.807410       99.461921      100.000000        1.176783\n",
      "         1       11.154252        6.908259        4.102036        2.806222       99.465461      100.000000        1.227777\n",
      "         1       11.157624        6.906018        4.101386        2.804632       99.468954      100.000000        1.163566\n",
      "         1      11.1612425        6.903963        4.101336        2.802627       99.462256      100.000000        1.241681\n",
      "         1       11.164612        6.901870        4.100086        2.801784       99.465726      100.000000        1.171899\n",
      "         1       11.167806        6.899817        4.099506        2.800311       99.469151      100.000000        1.191845\n",
      "         1       11.171209        6.897368        4.098239        2.799130       99.472532      100.000000        1.211611\n",
      "         1       11.174096        6.895505        4.097729        2.797776       99.475870      100.000000        1.188798\n",
      "         1       11.177608        6.893729        4.097532        2.796197       99.469340      100.000000        1.240725\n",
      "         1       11.181161        6.891160        4.096250        2.794910       99.472656      100.000000        1.184549\n",
      "         1       11.184712        6.889156        4.095127        2.794029       99.475932      100.000000        1.157083\n",
      "         1       11.188344        6.886657        4.093882        2.792774       99.469522      100.000000        1.163397\n",
      "         1       11.191879        6.884515        4.093281        2.791234       99.472776      100.000000        1.185065\n",
      "         1       11.195023        6.882605        4.092771        2.789833       99.456936      100.000000        1.138815\n",
      "         1       11.198337        6.880384        4.090638        2.789745       99.460227      100.000000        1.157647\n",
      "         1       11.201551        6.878433        4.089806        2.788627       99.463479      100.000000        1.198244\n",
      "         1       11.205471        6.876402        4.089432        2.786970       99.466692      100.000000        1.314881\n",
      "         1        11.20872        6.874154        4.088354        2.785800       99.460565      100.000000        1.571274\n",
      "         1       11.212139        6.871818        4.087504        2.784314       99.454512      100.000000        1.794888\n",
      "         1       11.215706        6.869526        4.085748        2.783778       99.457721      100.000000        1.700456\n",
      "         1       11.219063        6.867532        4.085232        2.782300       99.460892      100.000000        1.468259\n",
      "         1       11.222091        6.865452        4.084167        2.781286       99.464026      100.000000        1.671083\n",
      "         1       11.225185        6.863348        4.082440        2.780908       99.467124      100.000000        1.810702\n",
      "         1       11.228859        6.861398        4.082096        2.779302       99.461207      100.000000        2.247364\n",
      "         1       11.231981        6.859326        4.081061        2.778265       99.464286      100.000000        1.726843\n",
      "         1       11.235668        6.857382        4.080695        2.776686       99.449574      100.000000        1.596942\n",
      "         1       11.238687        6.855477        4.080133        2.775344       99.452684      100.000000        1.617615\n",
      "         1       11.241943        6.853369        4.079459        2.773910       99.455758      100.000000        1.779312\n",
      "         1       11.245191        6.851200        4.079025        2.772175       99.458799      100.000000        1.875952\n",
      "         1       11.248245        6.849244        4.078737        2.770508       99.461806      100.000000        1.903868\n",
      "         1       11.251566        6.847441        4.078384        2.769058       99.456146      100.000000        1.145492\n",
      "         1       11.254405        6.845334        4.077541        2.767793       99.459135      100.000000        1.203449\n",
      "         1       11.257752        6.843004        4.076054        2.766950       99.462090      100.000000        1.277011\n",
      "         1       11.260977        6.840921        4.075373        2.765548       99.448030      100.000000        1.257916\n",
      "         1       11.264228        6.839064        4.074600        2.764464       99.451014      100.000000        1.635721\n",
      "         1       11.267941        6.836768        4.074148        2.762620       99.453965      100.000000        1.544272\n",
      "         1       11.271379        6.834719        4.073544        2.761175       99.456885      100.000000        1.636837\n",
      "         1       11.274453        6.832794        4.073249        2.759545       99.459774      100.000000        1.542462\n",
      "         1       11.278066        6.830582        4.072527        2.758055       99.462632      100.000000        1.727152\n",
      "         1       11.281169        6.828336        4.071598        2.756738       99.465461      100.000000        1.407671\n",
      "         1       11.284198        6.826438        4.070998        2.755440       99.468259      100.000000        1.216797\n",
      "         1       11.287742        6.824183        4.070527        2.753656       99.454753      100.000000        1.197593\n",
      "         1       11.290821        6.822316        4.069436        2.752880       99.449482      100.000000        1.242579\n",
      "         1       11.293752        6.820375        4.069057        2.751318       99.452320      100.000000        1.227668\n",
      "         1       11.297474        6.818005        4.067352        2.750653       99.455128      100.000000        1.326453\n",
      "         1       11.300977        6.816055        4.066504        2.749551       99.449936      100.000000        1.296211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1       11.304054        6.813914        4.065421        2.748493       99.452728      100.000000        1.376856\n",
      "         1       11.307289        6.811770        4.064031        2.747739       99.455492      100.000000        1.129026\n",
      "         1       11.310238        6.809693        4.062698        2.746994       99.458229      100.000000        1.148486\n",
      "         1       11.313611        6.807595        4.062014        2.745581       99.445312      100.000000        1.256677\n",
      "         1       11.317069        6.805533        4.061367        2.744167       99.448072      100.000000        1.285536\n",
      "         1       11.320395        6.803465        4.060727        2.742738       99.450804      100.000000        1.547334\n",
      "         1       11.323851        6.801349        4.059702        2.741646       99.453510      100.000000        1.294603\n",
      "         1       11.327776        6.799063        4.058687        2.740376       99.456189      100.000000        1.230280\n",
      "         1       11.331015        6.797015        4.058019        2.738995       99.458841      100.000000        1.189076\n",
      "         1       11.334232        6.794942        4.056991        2.737952       99.461468      100.000000        1.113095\n",
      "         1       11.337591        6.792757        4.056174        2.736584       99.464070      100.000000        1.151312\n",
      "         1       11.340933        6.790515        4.054701        2.735814       99.466647      100.000000        1.231356\n",
      "         1        11.34424        6.788464        4.053432        2.735032       99.469199      100.000000        1.270676\n",
      "         1       11.347577        6.786402        4.052714        2.733688       99.471726      100.000000        1.674480\n",
      "         1       11.351113        6.783992        4.050869        2.733123       99.474230      100.000000        2.182081\n",
      "         1       11.354278        6.782049        4.048825        2.733224       99.469340      100.000000        1.470515\n",
      "         1       11.357525        6.780107        4.047365        2.732742       99.471831      100.000000        1.892208\n",
      "         1        11.36066        6.778192        4.046237        2.731955       99.474299      100.000000        1.685992\n",
      "         1       11.364032        6.776149        4.045012        2.731137       99.469477      100.000000        1.792585\n",
      "         1       11.366975        6.773924        4.042959        2.730965       99.471933      100.000000        1.223195\n",
      "         1       11.370392        6.771666        4.041300        2.730366       99.459965      100.000000        1.270100\n",
      "         1         11.3733        6.769453        4.040385        2.729069       99.462443      100.000000        1.210852\n",
      "         1       11.376797        6.767258        4.039588        2.727670       99.457763      100.000000        1.234615\n",
      "         1       11.380295        6.764698        4.037087        2.727611       99.460227      100.000000        1.301124\n",
      "         1       11.383456        6.762514        4.035674        2.726840       99.462670      100.000000        1.766741\n",
      "         1       11.386937        6.760325        4.034878        2.725447       99.458052      100.000000        1.145243\n",
      "         1        11.39006        6.758275        4.033595        2.724680       99.460482      100.000000        1.148915\n",
      "         1       11.393636        6.756052        4.032811        2.723242       99.455915      100.000000        1.245364\n",
      "         1       11.396982        6.753950        4.032334        2.721616       99.458333      100.000000        1.260053\n",
      "         1       11.399986        6.751789        4.030947        2.720842       99.460730      100.000000        1.319875\n",
      "         1       11.403624        6.749577        4.030433        2.719145       99.463106      100.000000        1.226683\n",
      "         1       11.407192        6.747503        4.029754        2.717750       99.458607      100.000000        1.155182\n",
      "         1       11.410269        6.745373        4.027340        2.718033       99.460972      100.000000        1.163973\n",
      "         1        11.41346        6.743278        4.026301        2.716977       99.463315      100.000000        1.193426\n",
      "         1      11.4167595        6.740969        4.025309        2.715659       99.465639      100.000000        1.119258\n",
      "         1       11.420045        6.738985        4.024388        2.714597       99.467942      100.000000        1.175386\n",
      "         1       11.423158        6.737121        4.024045        2.713076       99.470225      100.000000        1.145742\n",
      "         1       11.426363        6.735005        4.023221        2.711785       99.472489      100.000000        1.470475\n",
      "         1       11.429781        6.732966        4.021792        2.711173       99.468085      100.000000        1.715221\n",
      "         1       11.433009        6.730807        4.019984        2.710822       99.470339      100.000000        1.231052\n",
      "         1       11.436522        6.728426        4.018233        2.710193       99.472574      100.000000        1.194734\n",
      "         1       11.439752        6.726520        4.016456        2.710064       99.468225      100.000000        1.192176\n",
      "         1       11.442871        6.724683        4.015882        2.708802       99.470450      100.000000        1.268521\n",
      "         1       11.446515        6.722728        4.014836        2.707892       99.472656      100.000000        1.944849\n",
      "         1       11.450003        6.720394        4.013093        2.707301       99.468361      100.000000        1.645680\n",
      "         1       11.453482        6.718296        4.012148        2.706149       99.470558      100.000000        1.359465\n",
      "         1       11.457192        6.715974        4.010831        2.705143       99.466307      100.000000        1.617124\n",
      "         1       11.460647        6.713759        4.010216        2.703543       99.468494      100.000000        1.634782\n",
      "         1       11.463654        6.711883        4.009336        2.702547       99.470663      100.000000        1.644785\n",
      "         1       11.467195        6.709524        4.007660        2.701864       99.472815      100.000000        1.822321\n",
      "         2       12.266594        6.179465        3.620081        2.559384      100.000000      100.000000        1.572663\n",
      "         2       12.258182        6.210042        3.733763        2.476279      100.000000      100.000000        1.444695\n",
      "         2        12.27493        6.179996        3.718255        2.461741       99.479167      100.000000        1.363341\n",
      "         2       12.259302        6.186314        3.741170        2.445144       99.609375      100.000000        1.251276\n",
      "         2        12.27519        6.197130        3.780543        2.416587       99.062500      100.000000        1.308775\n",
      "         2       12.266064        6.202588        3.768938        2.433650       99.218750      100.000000        1.815579\n",
      "         2       12.279205        6.204954        3.781950        2.423004       99.107143      100.000000        1.524097\n",
      "         2       12.290536        6.202376        3.751194        2.451182       99.218750      100.000000        1.554482\n",
      "         2       12.291899        6.196576        3.736157        2.460419       99.305556      100.000000        1.301458\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m attack_traffic \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(batch_attack[idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_attack)])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#  Train Generator\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_G\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNOISE_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_traffic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_category\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m g_loss\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     63\u001b[0m run_g_loss\u001b[38;5;241m.\u001b[39mappend(g_loss)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtrain_generator\u001b[0;34m(generator, discriminator, optimizer_G, noise_dim, attack_traffic, attack_category)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_generator\u001b[39m(generator, discriminator, optimizer_G, noise_dim, attack_traffic, attack_category):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m#  for p in discriminator.layers:\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m#      p.trainable = False\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m#D_pred = tf.constant(D_pred, dtype=tf.float32)\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m#  with tf.GradientTape(persistent = False) as Gtape:\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m Gtape:\n\u001b[0;32m--> 370\u001b[0m         adversarial_attack, generator_out \u001b[38;5;241m=\u001b[39m \u001b[43mgen_adversarial_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_traffic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_category\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m         D_pred \u001b[38;5;241m=\u001b[39m discriminator(adversarial_attack)\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;66;03m#  Gtape.watch(D_pred)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mgen_adversarial_attack\u001b[0;34m(generator, noise_dim, raw_attack, attack_category)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(raw_attack\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(not_pos_nonfunctional_feature)):\n\u001b[0;32m--> 223\u001b[0m         keep_attack[idx, j] \u001b[38;5;241m=\u001b[39m \u001b[43mraw_attack\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[not_pos_nonfunctional_feature[j]]\n\u001b[1;32m    225\u001b[0m adversarial_attack \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([keep_attack, generator_out], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    226\u001b[0m adversarial_attack \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatmul(tf\u001b[38;5;241m.\u001b[39mconstant(p_matrix, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32), adversarial_attack, transpose_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1033\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1030\u001b[0m   index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# stack possibly involves no tensors, so we must use op_scope correct graph.\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrided_slice\u001b[39m\u001b[38;5;124m\"\u001b[39m, [tensor] \u001b[38;5;241m+\u001b[39m begin \u001b[38;5;241m+\u001b[39m end \u001b[38;5;241m+\u001b[39m strides,\n\u001b[1;32m   1036\u001b[0m     skip_on_eager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m   1037\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m begin:\n\u001b[1;32m   1038\u001b[0m     packed_begin, packed_end, packed_strides \u001b[38;5;241m=\u001b[39m (stack(begin), stack(end),\n\u001b[1;32m   1039\u001b[0m                                                 stack(strides))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:6945\u001b[0m, in \u001b[0;36mname_scope_v2.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6941\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   6942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   6943\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n\u001b[0;32m-> 6945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   6946\u001b[0m   \u001b[38;5;124;03m\"\"\"Start the scope block.\u001b[39;00m\n\u001b[1;32m   6947\u001b[0m \n\u001b[1;32m   6948\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m   6949\u001b[0m \u001b[38;5;124;03m    The scope name.\u001b[39;00m\n\u001b[1;32m   6950\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m   6951\u001b[0m   ctx \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcontext()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from datetime import date\n",
    "import timeit\n",
    "\n",
    "print(f\"{40*'='} TRAINING GAN {40*'='}\")\n",
    "print(f\"{80*'='}\\n IDS Model: {'BiLstm'}\")\n",
    "NOISE_DIM = 9\n",
    "\n",
    "for attack_category in ATTACK_CATEGORIES:\n",
    "    print(f\"{80*'-'}\\n    Attack Category: {attack_category}\")\n",
    "    total_time_start = timeit.default_timer()\n",
    "    evasion_each = []\n",
    "\n",
    "    # Init GAN-G model\n",
    "    G_OUTPUT_DIM = len(POS_NONFUNCTIONAL_FEATURES[attack_category])     # Generator output is number of nonfunctional feature\n",
    "    print(f\"    nf              : {G_OUTPUT_DIM} (num. of nonfunctional features)\")\n",
    "    NOISE_DIM = G_OUTPUT_DIM\n",
    "    G_INPUT_DIM = NOISE_DIM\n",
    "    print(f\"    GAN-G NOISE DIM : {NOISE_DIM}\")\n",
    "    print(f\"    GAN-G INPUT DIM : {G_INPUT_DIM}\")\n",
    "    print(f\"    GAN-G OUTPUT DIM: {G_OUTPUT_DIM}\")\n",
    "    generator = init_generator(attack_category)\n",
    "    discriminator = Discriminator(D_INPUT_DIM)\n",
    "\n",
    "\n",
    "    #  generator.summary()\n",
    "    optimizer_G = tf.keras.optimizers.RMSprop(6e-6)\n",
    "    optimizer_D = tf.keras.optimizers.RMSprop(6e-6)\n",
    "    # Load Raw Attack Dataset\n",
    "\n",
    "    raw_attack = preprocess_adversarial_data(g_train_data, attack_category)\n",
    "    # Create batch of attack traffic\n",
    "    batch_attack = create_batch2(raw_attack, BATCH_SIZE)\n",
    "    # Declare Loss, DR List and Train GAN-G, GAN-D\n",
    "    d_losses,g_losses = [],[]\n",
    "    o_dr, a_dr = [],[]\n",
    "    # Start Training\n",
    "    print(f\"==>IDSGAN start training\")\n",
    "    print(f\"{4*' '}Folder for saving GAN Models : {GAN_Save_Path}\")\n",
    "    labels = ['Epoch', 'G-Loss', 'D-Loss', 'Normal-loss','Benign-loss', 'O-DR(%)', 'A-DR(%)', 'Runtime(s)']\n",
    "    print(\"{: >10} {: >15} {: >15} {: >15} {: >15} {: >15} {: >15} {: >15}\".format(*labels))\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        batch_normal = create_batch2(normal,BATCH_SIZE)\n",
    "        epoch_time_start = timeit.default_timer()\n",
    "        cnt = 0\n",
    "        run_g_loss = []\n",
    "        run_d_loss = []\n",
    "        run_loss_normal = []\n",
    "        run_loss_attack = []\n",
    "        epoch_o_drs, epoch_a_drs = [], []\n",
    "\n",
    "        for idx, bn in enumerate(batch_normal):\n",
    "            batch_time = timeit.default_timer()\n",
    "            normal_b = tf.convert_to_tensor(bn, dtype = np.float64)\n",
    "            attack_traffic = tf.convert_to_tensor(batch_attack[idx % len(batch_attack)])\n",
    "\n",
    "            #  Train Generator\n",
    "\n",
    "            g_loss = train_generator(generator, discriminator, optimizer_G, NOISE_DIM, attack_traffic, attack_category)\n",
    "            g_loss = g_loss.numpy()\n",
    "            run_g_loss.append(g_loss)\n",
    "            \n",
    "\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_loss, loss_normal, loss_attack, current_cnt = train_discriminator(discriminator, ids_model, generator, CRITIC_ITERS, optimizer_D, normal_b, NOISE_DIM, attack_traffic, attack_category)\n",
    "            run_d_loss.append(d_loss)\n",
    "            run_loss_normal.append(loss_normal)\n",
    "            run_loss_attack.append(loss_attack)\n",
    "            cnt += current_cnt\n",
    "\n",
    "            # CALC Epoch DR\n",
    "\n",
    "            adversarial_attack, generator_out = gen_adversarial_attack(generator, NOISE_DIM, attack_traffic, attack_category)\n",
    "\n",
    "            gen_loss = np.mean(run_g_loss)\n",
    "            dis_loss = np.mean(run_d_loss)\n",
    "            dis_n_loss = np.mean(run_loss_normal)\n",
    "            dis_a_loss = np.mean(run_loss_attack)\n",
    "            origin_dr, adversarial_dr = cal_dr(ids_model, normal_b, attack_traffic, adversarial_attack)\n",
    "            evasion_rate = origin_dr - adversarial_dr\n",
    "            evasion_each.append(evasion_rate)\n",
    "            epoch_o_drs.append(origin_dr)\n",
    "            epoch_a_drs.append(adversarial_dr)\n",
    "            epoch_o_dr = np.mean(epoch_o_drs)\n",
    "            epoch_a_dr = np.mean(epoch_a_drs)\n",
    "            o_dr.append(epoch_o_dr)\n",
    "            a_dr.append(epoch_a_dr)\n",
    "            g_losses.append(gen_loss)\n",
    "            d_losses.append(dis_loss)\n",
    "\n",
    "            runtime = timeit.default_timer() - batch_time\n",
    "            print_vals = [(epoch + 1), gen_loss, (dis_loss / CRITIC_ITERS), (dis_n_loss / CRITIC_ITERS), (dis_a_loss / CRITIC_ITERS), (epoch_o_dr*100), (epoch_a_dr*100), runtime]\n",
    "            print_string = []\n",
    "            for val in print_vals:\n",
    "                if isinstance(val, float):\n",
    "                    print_string.append(str(f\"{val:.6f}\"))\n",
    "                else:\n",
    "                    print_string.append(str(val))\n",
    "            print(\"{: >10} {: >15} {: >15} {: >15} {: >15} {: >15} {: >15} {: >15}\".format(*print_string))\n",
    "\n",
    "        # Save Adversarial Dataset each 10 epoch\n",
    "\n",
    "    total_runtime = timeit.default_timer() - total_time_start\n",
    "    print(f\"Training Runtime: {total_runtime:.2f}\")\n",
    "    print(\"IDSGAN finish training!\")\n",
    "    EVASION_RATE[attack_category] = evasion_each\n",
    "    # Show Graph\n",
    "    # Loss-Graph\n",
    "    plt.plot(d_losses,label = \"D_loss\")\n",
    "    plt.plot(g_losses, label = \"G_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # DR-Graph\n",
    "    plt.plot(o_dr,label = \"Origin DR\")\n",
    "    plt.plot(a_dr, label = \"Adversarial DR\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Adversarial Traffic Evaluating\n",
    "    print(f\"{40*'='} ADVERSARIAL TRAFFIC EVALUATING {40*'='}\")\n",
    "    print(f\"{80*'='}\\n IDS Model: CNN\")\n",
    "\n",
    "    # Load Attack Dataset\n",
    "    test_raw_attack = preprocess_adversarial_data(testset, attack_category)\n",
    "    # Create batch of attack traffic\n",
    "    batch_attack = create_batch2(test_raw_attack, BATCH_SIZE)\n",
    "    n_batch_attack = len(batch_attack)\n",
    "    print(f\"{4*' '}Amout of {attack_category}:\\t{len(test_raw_attack)} ({n_batch_attack} batchs - {BATCH_SIZE} records/batch)\")\n",
    "\n",
    "    # Calc DR through each epoch\n",
    "\n",
    "    for epoch in range(0, MAX_EPOCH + 1, 10):\n",
    "        # Load GAN-G Model\n",
    "        o_dr,a_dr =[],[]\n",
    "        generated_dataset = pd.DataFrame(columns=DATASET_COLUMNS)\n",
    "\n",
    "        for idx, bn in enumerate(test_batch_normal):\n",
    "            normal_b = tf.convert_to_tensor(bn)\n",
    "            attack_b = tf.convert_to_tensor(batch_attack[idx % n_batch_attack])\n",
    "            # Generate Adversarial Traffic\n",
    "            adversarial_attack_b,generator_out = gen_adversarial_attack(generator, NOISE_DIM, attack_b, attack_category)\n",
    "            if (idx < n_batch_attack):\n",
    "                epoch_dataset = pd.DataFrame(data = np.array(adversarial_attack_b), columns=DATASET_COLUMNS)\n",
    "                generated_dataset = pd.concat([generated_dataset,epoch_dataset], ignore_index = True)\n",
    "\n",
    "            # Calc DR\n",
    "            origin_dr, adversarial_dr = cal_dr(ids_model, normal_b, attack_b, adversarial_attack_b)\n",
    "            o_dr.append(origin_dr)\n",
    "            a_dr.append(adversarial_dr)\n",
    "        eir = 1 - (np.mean(a_dr)/np.mean(o_dr))\n",
    "        print(f\"\\t {epoch:3d} epochs:\\tOrigin DR : {np.mean(o_dr)*100:.2f}% \\t Adversarial DR : {np.mean(a_dr)*100:.2f}% \\t EIR : {eir*100:.2f}%\")\n",
    "        if epoch == MAX_EPOCH:\n",
    "            generated_dataset_folder_path = str(f\"{generated_dataset_path}BiLstm/{attack_category}/\")\n",
    "            if not os.path.exists(generated_dataset_folder_path):\n",
    "                os.makedirs(generated_dataset_folder_path)\n",
    "            generated_dataset_file_path = generated_dataset_folder_path + str(f\"CNN_{attack_category}.csv\")\n",
    "            save_generated_dataset(generated_dataset, generated_dataset_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = [100]\n",
    "def get_cat_vals(dataset, attack_category):\n",
    "    cat_num, cat_vals = [], []\n",
    "    for nonfunctional_feature in  POS_NONFUNCTIONAL_FEATURES[attack_category]:\n",
    "        np_cat_val = dataset[:, nonfunctional_feature]\n",
    "        cat_val = pd.Series(np_cat_val)\n",
    "        cat_num.append(len(cat_val.unique()))\n",
    "        cat_vals.append(cat_val.unique())\n",
    "    return cat_num, cat_vals\n",
    "\n",
    "# count_different_element - count number of element of adversarials that not in originals\n",
    "def count_different_element(adversarials_set, originals_set):\n",
    "    count_set = []\n",
    "    for idx, adversarials in enumerate(adversarials_set):\n",
    "        originals = originals_set[idx]\n",
    "        count = 0\n",
    "        for adversarial in adversarials:\n",
    "            if adversarial not in originals:\n",
    "                count+= 1\n",
    "        count_set.append(count)\n",
    "    return count_set\n",
    "# Pandas Display Setting\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for attack_category in ATTACK_CATEGORIES:\n",
    "    \n",
    "    print(f\"<{80*'='}>\\n  # Attack Category: {attack_category}\")\n",
    "\n",
    "    # compare_table - init compare_table: Create Column Nonfunctional Feature\n",
    "    data = {'Nonfunctional_Feature':DATASET_COLUMNS[POS_NONFUNCTIONAL_FEATURES[attack_category]]}\n",
    "    compare_table = pd.DataFrame(data)\n",
    "\n",
    "    # compare_table - compare_table add column Origin Number of Category and Origin Categories\n",
    "    test_raw_attack = preprocess_adversarial_data(testset, attack_category)\n",
    "    cat_num, cat_vals = get_cat_vals(test_raw_attack, attack_category)\n",
    "    compare_table['Origin-Num_Cat'] = cat_num\n",
    "    compare_table['Origin-Cat'] = cat_vals\n",
    "\n",
    "    for epoch in EPOCHS:\n",
    "        print(f\"    --> Computing in Epoch: {epoch}...\")\n",
    "        # Load Adversarial Attack Dataset\n",
    "        adversarial_set_path = str(f\"{generated_dataset_path}CNN/{attack_category}/\") + str(f\"CNN_{attack_category}.csv\")\n",
    "        adversarial_set = pd.read_csv(adversarial_set_path)\n",
    "        adversarial_attack = np.array(adversarial_set)\n",
    "        print(f\"         Adversarial Attack Data Shape: {adversarial_attack.shape}\")\n",
    "\n",
    "        # compare_table - compare_table add column Adversarial Categories of each Epoch\n",
    "        ad_cat_num, ad_cat_vals = get_cat_vals(adversarial_attack, attack_category)\n",
    "        compare_table[str(f'E{epoch}-Num_Adversarial_Category')] = ad_cat_num\n",
    "        compare_table[str(f'E{epoch}-Adversarial_Category')] = ad_cat_vals\n",
    "        compare_table[str(f'E{epoch}-Num_NotIn_Origin')] = count_different_element(ad_cat_vals, cat_vals)\n",
    "    print_columns = []\n",
    "    print_columns.append('Nonfunctional_Feature')\n",
    "    print_columns.append('Origin-Num_Cat')\n",
    "\n",
    "    for e in EPOCHS:\n",
    "        print_columns.append(str(f\"E{epoch}-Num_Adversarial_Category\"))\n",
    "        print_columns.append(str(f\"E{epoch}-Num_NotIn_Origin\"))\n",
    "\n",
    "    # Print the Statistic\n",
    "    print(80*'-')\n",
    "    print(compare_table[print_columns].to_markdown())\n",
    "\n",
    "    for epoch in EPOCHS:\n",
    "        not_in_column = str(f\"E{epoch}-Num_NotIn_Origin\")\n",
    "        print(f\"\\n ==> E{epoch}: Num. of feature not exist before: {len(compare_table[not_in_column][compare_table[not_in_column] != 0])}/{len(compare_table[not_in_column])}\")\n",
    "\n",
    "        # Save the Statistic\n",
    "        folder_save_path = os.path.dirname(adversarial_set_path)\n",
    "        statistic_file_name = str(f'Statistic_of_E{epoch}_dataset_.csv')\n",
    "        compare_table.to_csv(folder_save_path +'/'+ statistic_file_name)\n",
    "        print(f\" The full statistic saved at: {folder_save_path}\")\n",
    "        print(f\" Name: {statistic_file_name}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3AVOx-f7RQSU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AVOx-f7RQSU",
    "outputId": "b6c8dfc8-a8ff-4300-9dae-74c26d62efa7"
   },
   "outputs": [],
   "source": [
    "DoS = EVASION_RATE['DoS']\n",
    "U2R = EVASION_RATE['U2R']\n",
    "R2L = EVASION_RATE['R2L']\n",
    "Probe = EVASION_RATE['Probe']\n",
    "np.save(SavedModelPath + 'DoS_CNN_F' , DoS)\n",
    "np.save(SavedModelPath + 'U2R_CNN_F' , U2R)\n",
    "np.save(SavedModelPath + 'R2L_CNN_F' , R2L)\n",
    "np.save(SavedModelPath + 'Probe_CNN_F' , Probe)\n",
    "\n",
    "os.listdir(SavedModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09_b8vcG5cUf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09_b8vcG5cUf",
    "outputId": "4806b660-84b6-4f4c-e9d3-91a50ee823b4"
   },
   "outputs": [],
   "source": [
    "generator.save(GAN_Save_Path,'generator_FGAN.h5')\n",
    "discriminator.save(GAN_Save_Path,'discriminator_FGAN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W31lfaysv_XU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W31lfaysv_XU",
    "outputId": "e16ab489-0581-4970-9432-d91fda984f4e"
   },
   "outputs": [],
   "source": [
    "EVASION_RATE['DoS']\n",
    "os.listdir()\n",
    "DoS = np.load(SavedModelPath + 'DoS_CNN.npy')\n",
    "U2R = np.load(SavedModelPath + 'U2R_CNN.npy')\n",
    "R2L = np.load(SavedModelPath + 'R2L_CNN.npy')\n",
    "Probe = np.load(SavedModelPath + 'Probe_CNN.npy')\n",
    "\n",
    "plt.figure(figsize=(20, 6.5))\n",
    "plt.plot(DoS, label = \"IDSGAN\")\n",
    "plt.plot(EVASION_RATE['DoS'], label = \"Feature_GAN\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 6.5))\n",
    "plt.plot(U2R, label = \"IDSGAN\")\n",
    "plt.plot(EVASION_RATE['U2R'], label = \"Feature_GAN\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 6.5))\n",
    "plt.plot(R2L, label = \"IDSGAN\")\n",
    "plt.plot(EVASION_RATE['R2L'], label = \"Feature_GAN\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 6.5))\n",
    "plt.plot(Probe, label = \"IDSGAN\")\n",
    "plt.plot(EVASION_RATE['Probe'], label = \"Feature_GAN\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M_JYYbhXwA8a",
   "metadata": {
    "id": "M_JYYbhXwA8a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN-Feature.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
